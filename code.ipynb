{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "0uBqWyl-b2wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use PyTorch for models, scikit-learn for preprocessing and metrics.\n",
        "\n",
        "import pandas as pd  # For data loading and manipulation\n",
        "import numpy as np  # For numerical operations\n",
        "from sklearn.model_selection import train_test_split  # For splitting data\n",
        "from sklearn.preprocessing import StandardScaler  # For scaling features\n",
        "from sklearn.metrics import r2_score, mean_squared_error  # For evaluation\n",
        "import torch  # For neural networks\n",
        "import torch.nn as nn  # For model layers\n",
        "import torch.optim as optim  # For optimizer\n",
        "from torch.utils.data import Dataset, DataLoader  # For data loading in PyTorch\n",
        "import matplotlib.pyplot as plt  # For plotting losses\n",
        "import os  # For directory creation and saving files\n",
        "import joblib  # For saving scaler\n",
        "\n",
        "# Device configuration: Use GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYXKqOFbbxZ6",
        "outputId": "82103916-827a-4da0-9da4-1480c0fefb89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (Finding answers for Step-1)"
      ],
      "metadata": {
        "id": "eXgOwxSCggrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset and print key statistics to answer report questions.\n",
        "\n",
        "# Load the dataset with specified encoding\n",
        "df = pd.read_csv('/content/cancer_reg-1.csv', encoding='latin-1')\n",
        "\n",
        "# Print basic info for Step 1\n",
        "print(\"Number of samples (Question 1):\", df.shape[0])\n",
        "print(\"Number of features (Question 4):\", df.shape[1] - 1)  # Excluding label\n",
        "print(\"Missing information (Question 5):\")\n",
        "print(df.isnull().sum())  # Shows missing per column\n",
        "print(\"Label (Question 6): TARGET_deathRate\")\n",
        "\n",
        "# Compute min and max in the dataset (Question 3)\n",
        "numerical_df = df.select_dtypes(include='number')\n",
        "min_val = numerical_df.min().min()\n",
        "max_val = numerical_df.max().max()\n",
        "print(\"Min value in dataset:\", min_val)\n",
        "print(\"Max value in dataset:\", max_val)\n",
        "\n",
        "# Describe for more insights\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QElTi5mGgx8D",
        "outputId": "1dfb5958-06dc-4dc5-d477-665c2573b16e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples (Question 1): 3047\n",
            "Number of features (Question 4): 33\n",
            "Missing information (Question 5):\n",
            "avgAnnCount                   0\n",
            "avgDeathsPerYear              0\n",
            "TARGET_deathRate              0\n",
            "incidenceRate                 0\n",
            "medIncome                     0\n",
            "popEst2015                    0\n",
            "povertyPercent                0\n",
            "studyPerCap                   0\n",
            "binnedInc                     0\n",
            "MedianAge                     0\n",
            "MedianAgeMale                 0\n",
            "MedianAgeFemale               0\n",
            "Geography                     0\n",
            "AvgHouseholdSize              0\n",
            "PercentMarried                0\n",
            "PctNoHS18_24                  0\n",
            "PctHS18_24                    0\n",
            "PctSomeCol18_24            2285\n",
            "PctBachDeg18_24               0\n",
            "PctHS25_Over                  0\n",
            "PctBachDeg25_Over             0\n",
            "PctEmployed16_Over          152\n",
            "PctUnemployed16_Over          0\n",
            "PctPrivateCoverage            0\n",
            "PctPrivateCoverageAlone     609\n",
            "PctEmpPrivCoverage            0\n",
            "PctPublicCoverage             0\n",
            "PctPublicCoverageAlone        0\n",
            "PctWhite                      0\n",
            "PctBlack                      0\n",
            "PctAsian                      0\n",
            "PctOtherRace                  0\n",
            "PctMarriedHouseholds          0\n",
            "BirthRate                     0\n",
            "dtype: int64\n",
            "Label (Question 6): TARGET_deathRate\n",
            "Min value in dataset: 0.0\n",
            "Max value in dataset: 10170292.0\n",
            "        avgAnnCount  avgDeathsPerYear  TARGET_deathRate  incidenceRate  \\\n",
            "count   3047.000000       3047.000000       3047.000000    3047.000000   \n",
            "mean     606.338544        185.965868        178.664063     448.268586   \n",
            "std     1416.356223        504.134286         27.751511      54.560733   \n",
            "min        6.000000          3.000000         59.700000     201.300000   \n",
            "25%       76.000000         28.000000        161.200000     420.300000   \n",
            "50%      171.000000         61.000000        178.100000     453.549422   \n",
            "75%      518.000000        149.000000        195.200000     480.850000   \n",
            "max    38150.000000      14010.000000        362.800000    1206.900000   \n",
            "\n",
            "           medIncome    popEst2015  povertyPercent  studyPerCap    MedianAge  \\\n",
            "count    3047.000000  3.047000e+03     3047.000000  3047.000000  3047.000000   \n",
            "mean    47063.281917  1.026374e+05       16.878175   155.399415    45.272333   \n",
            "std     12040.090836  3.290592e+05        6.409087   529.628366    45.304480   \n",
            "min     22640.000000  8.270000e+02        3.200000     0.000000    22.300000   \n",
            "25%     38882.500000  1.168400e+04       12.150000     0.000000    37.700000   \n",
            "50%     45207.000000  2.664300e+04       15.900000     0.000000    41.000000   \n",
            "75%     52492.000000  6.867100e+04       20.400000    83.650776    44.000000   \n",
            "max    125635.000000  1.017029e+07       47.400000  9762.308998   624.000000   \n",
            "\n",
            "       MedianAgeMale  ...  PctPrivateCoverageAlone  PctEmpPrivCoverage  \\\n",
            "count    3047.000000  ...              2438.000000         3047.000000   \n",
            "mean       39.570725  ...                48.453774           41.196324   \n",
            "std         5.226017  ...                10.083006            9.447687   \n",
            "min        22.400000  ...                15.700000           13.500000   \n",
            "25%        36.350000  ...                41.000000           34.500000   \n",
            "50%        39.600000  ...                48.700000           41.100000   \n",
            "75%        42.500000  ...                55.600000           47.700000   \n",
            "max        64.700000  ...                78.900000           70.700000   \n",
            "\n",
            "       PctPublicCoverage  PctPublicCoverageAlone     PctWhite     PctBlack  \\\n",
            "count        3047.000000             3047.000000  3047.000000  3047.000000   \n",
            "mean           36.252642               19.240072    83.645286     9.107978   \n",
            "std             7.841741                6.113041    16.380025    14.534538   \n",
            "min            11.200000                2.600000    10.199155     0.000000   \n",
            "25%            30.900000               14.850000    77.296180     0.620675   \n",
            "50%            36.300000               18.800000    90.059774     2.247576   \n",
            "75%            41.550000               23.100000    95.451693    10.509732   \n",
            "max            65.100000               46.600000   100.000000    85.947799   \n",
            "\n",
            "          PctAsian  PctOtherRace  PctMarriedHouseholds    BirthRate  \n",
            "count  3047.000000   3047.000000           3047.000000  3047.000000  \n",
            "mean      1.253965      1.983523             51.243872     5.640306  \n",
            "std       2.610276      3.517710              6.572814     1.985816  \n",
            "min       0.000000      0.000000             22.992490     0.000000  \n",
            "25%       0.254199      0.295172             47.763063     4.521419  \n",
            "50%       0.549812      0.826185             51.669941     5.381478  \n",
            "75%       1.221037      2.177960             55.395132     6.493677  \n",
            "max      42.619425     41.930251             78.075397    21.326165  \n",
            "\n",
            "[8 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing (Convert **binnedInc** format)"
      ],
      "metadata": {
        "id": "wyudlI_ghXDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process the data: drop unnecessary columns, convert binnedInc, fill missing with train means,\n",
        "# split into train/val/test, scale features, save CSVs and scaler.\n",
        "\n",
        "# Define function to parse binnedInc to midpoint\n",
        "def parse_bin(s):\n",
        "    s = s.replace('[', '').replace(']', '').replace('(', '').replace(')', '')\n",
        "    parts = s.split(',')\n",
        "    if len(parts) == 2:\n",
        "        low = float(parts[0].strip())\n",
        "        high = float(parts[1].strip())\n",
        "        return (low + high) / 2\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "# Drop Geography (unique categorical, not useful)\n",
        "df = df.drop('Geography', axis=1)\n",
        "\n",
        "# Convert binnedInc\n",
        "df['binnedInc'] = df['binnedInc'].apply(parse_bin)\n",
        "\n",
        "# Split indices for reproducibility\n",
        "train_idx, temp_idx = train_test_split(df.index, test_size=0.3, random_state=42)\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
        "\n",
        "# Create split DataFrames\n",
        "df_train = df.loc[train_idx].copy()\n",
        "df_val = df.loc[val_idx].copy()\n",
        "df_test = df.loc[test_idx].copy()\n",
        "\n",
        "# Save splits as CSVs (full, with labels)\n",
        "df_train.to_csv('train.csv', index=False, encoding='latin-1')\n",
        "df_val.to_csv('val.csv', index=False, encoding='latin-1')\n",
        "df_test.to_csv('test.csv', index=False, encoding='latin-1')\n",
        "\n",
        "# Fill missing with train means\n",
        "for col in df_train.columns:\n",
        "    if df_train[col].dtype in ['float64', 'int64'] and df_train[col].isnull().any():\n",
        "        train_mean = df_train[col].mean()\n",
        "        df_train[col] = df_train[col].fillna(train_mean)\n",
        "        df_val[col] = df_val[col].fillna(train_mean)\n",
        "        df_test[col] = df_test[col].fillna(train_mean)\n",
        "\n",
        "# Separate features and labels\n",
        "X_train = df_train.drop('TARGET_deathRate', axis=1).values\n",
        "y_train = df_train['TARGET_deathRate'].values\n",
        "X_val = df_val.drop('TARGET_deathRate', axis=1).values\n",
        "y_val = df_val['TARGET_deathRate'].values\n",
        "X_test = df_test.drop('TARGET_deathRate', axis=1).values\n",
        "y_test = df_test['TARGET_deathRate'].values\n",
        "\n",
        "# Scale features (fit on train)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Save scaler for test_model\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "# Custom Dataset class\n",
        "class CancerDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create datasets and loaders (larger batch for GPU)\n",
        "train_dataset = CancerDataset(X_train, y_train)\n",
        "val_dataset = CancerDataset(X_val, y_val)\n",
        "test_dataset = CancerDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "print(\"Pre-processing complete. Input size:\", input_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_uRIkSshWoO",
        "outputId": "4a5a08c5-c257-40c4-bae2-964f5f2cc5f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-processing complete. Input size: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Model Class"
      ],
      "metadata": {
        "id": "OqU26dvtqe1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DNN class for linear (no hidden) and deep/wide networks.\n",
        "\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers=[]):\n",
        "        super(DNN, self).__init__()\n",
        "        layers = []\n",
        "        in_size = input_size\n",
        "        for h in hidden_layers:\n",
        "            layers.append(nn.Linear(in_size, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            in_size = h\n",
        "        layers.append(nn.Linear(in_size, 1))  # Regression output\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "CnYqViwAmp4B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation Functions"
      ],
      "metadata": {
        "id": "83BRX4Rwqy6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with early stopping, gradient clipping. Evaluate and handle NaN.\n",
        "\n",
        "def train_model(model, train_loader, val_loader, lr=0.01, epochs=500, loss_fn=nn.MSELoss(), patience=50):\n",
        "    model.to(device)\n",
        "    criterion = loss_fn\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in train_loader:\n",
        "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            if torch.isnan(loss).any():\n",
        "                print(f\"NaN loss at epoch {epoch+1}. Stopping.\")\n",
        "                return train_losses, val_losses\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                inputs, labels = batch[0].to(device), batch[1].to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # Print every 50 epochs for screenshots\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
        "            outputs = model(inputs)\n",
        "            y_pred.extend(outputs.cpu().numpy().flatten())\n",
        "            y_true.extend(labels.cpu().numpy().flatten())\n",
        "    if np.isnan(y_pred).any():\n",
        "        print(\"NaN in predictions.\")\n",
        "        return float('inf'), float('-inf')\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"Test MSE: {mse:.4f}, Test R2: {r2:.4f}\")\n",
        "    return mse, r2\n",
        "\n",
        "def test_model(model_path, test_csv_path, hidden_layers, input_size=32):\n",
        "    model = DNN(input_size, hidden_layers)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_df = pd.read_csv(test_csv_path, encoding='latin-1')\n",
        "    if 'Geography' in test_df.columns:\n",
        "        test_df = test_df.drop('Geography', axis=1)\n",
        "    test_df['binnedInc'] = test_df['binnedInc'].apply(parse_bin)\n",
        "    for col in test_df.columns:\n",
        "        if test_df[col].dtype in ['float64', 'int64'] and test_df[col].isnull().any():\n",
        "            test_df[col] = test_df[col].fillna(test_df[col].mean())\n",
        "    scaler = joblib.load('scaler.pkl')\n",
        "    X_test_new = scaler.transform(test_df.values)\n",
        "    test_tensor = torch.tensor(X_test_new, dtype=torch.float32).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_tensor).cpu().numpy().flatten()\n",
        "    print(\"Predictions:\", predictions)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "NSBgk5WLqyVl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Evaluate All Models (Answers for steps 2,5,6)"
      ],
      "metadata": {
        "id": "szur7GIurUbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 13 architectures: 5 mandatory DNN + linear, 4 wide, 4 deep.\n",
        "# Train with different LR, save plots for LR=0.01, find best, save models.\n",
        "\n",
        "# Create plots folder\n",
        "os.makedirs('/content/plots', exist_ok=True)\n",
        "\n",
        "# Architectures\n",
        "architectures = {\n",
        "    'Linear': [],\n",
        "    'DNN-16': [16],\n",
        "    'DNN-30-8': [30, 8],\n",
        "    'DNN-30-16-8': [30, 16, 8],\n",
        "    'DNN-30-16-8-4': [30, 16, 8, 4],\n",
        "    'DNN-128': [128],  # Wide 1\n",
        "    'DNN-256-128': [256, 128],  # Wide 2\n",
        "    'DNN-512-256-128': [512, 256, 128],  # Wide 3\n",
        "    'DNN-1024-512-256-128': [1024, 512, 256, 128],  # Wide 4\n",
        "    'DNN-32x4': [32] * 4,  # Deep 1\n",
        "    'DNN-16x5': [16] * 5,  # Deep 2\n",
        "    'DNN-8x6': [8] * 6,  # Deep 3\n",
        "    'DNN-64-32-16-8-4': [64, 32, 16, 8, 4],  # Deep 4\n",
        "}\n",
        "\n",
        "# Learning rates\n",
        "lrs = [0.1, 0.01, 0.001, 0.0001]\n",
        "\n",
        "# Results storage\n",
        "step2_r2 = {}  # LR=0.01\n",
        "step5_r2 = {name: {} for name in architectures}\n",
        "step6_mse = {}  # LR=0.01\n",
        "best_model_name = None\n",
        "best_r2 = -float('inf')\n",
        "best_hidden_layers = []\n",
        "\n",
        "# Train loop\n",
        "for model_name, hidden_layers in architectures.items():\n",
        "    print(f\"\\nTraining {model_name}\")\n",
        "    for lr in lrs:\n",
        "        model = DNN(input_size, hidden_layers)\n",
        "        train_losses, val_losses = train_model(model, train_loader, val_loader, lr=lr, epochs=500)\n",
        "        mse, r2 = evaluate_model(model, test_loader)\n",
        "        step5_r2[model_name][lr] = r2\n",
        "        if lr == 0.01:\n",
        "            step2_r2[model_name] = r2\n",
        "            step6_mse[model_name] = mse\n",
        "\n",
        "            # Save plot with caption\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(train_losses, label='Train Loss')\n",
        "            plt.plot(val_losses, label='Val Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('MSE Loss')\n",
        "            plt.title(f'Loss Curve for {model_name}')\n",
        "            plt.legend()\n",
        "            caption = f\"Training and Validation Loss for {model_name} over {len(train_losses)} epochs (LR=0.01).\\nFinal Train Loss: {train_losses[-1]:.4f}, Final Val Loss: {val_losses[-1]:.4f}.\\nThis plot shows convergence; early stopping applied if no improvement for 50 epochs.\"\n",
        "            plt.figtext(0.5, -0.1, caption, wrap=True, ha='center', fontsize=10)\n",
        "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "            plt.savefig(f'/content/plots/{model_name}_loss.png', bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            # Track best\n",
        "            if r2 > best_r2:\n",
        "                best_r2 = r2\n",
        "                best_model_name = model_name\n",
        "                best_hidden_layers = hidden_layers\n",
        "                torch.save(model.state_dict(), 'best_dnn.pth')\n",
        "\n",
        "    # Save linear separately\n",
        "    if model_name == 'Linear':\n",
        "        torch.save(model.state_dict(), 'linear.pth')\n",
        "\n",
        "# Print results\n",
        "print(\"\\nStep 2 Results (R2 for LR=0.01):\")\n",
        "for name, r2 in step2_r2.items():\n",
        "    print(f\"{name}: {r2:.4f}\")\n",
        "\n",
        "print(\"\\nStep 5 Results (R2 for different LR):\")\n",
        "for name, lr_dict in step5_r2.items():\n",
        "    print(name)\n",
        "    for lr, r2 in lr_dict.items():\n",
        "        print(f\"LR {lr}: {r2:.4f}\")\n",
        "\n",
        "print(\"\\nStep 6 MSE (for LR=0.01):\")\n",
        "for name, mse in step6_mse.items():\n",
        "    print(f\"{name}: {mse:.4f}\")\n",
        "\n",
        "# Try MAE for DNN-30-16-8, LR=0.01\n",
        "print(\"\\nTrying MAE loss for DNN-30-16-8\")\n",
        "model_mae = DNN(input_size, [30, 16, 8])\n",
        "train_losses_mae, val_losses_mae = train_model(model_mae, train_loader, val_loader, lr=0.01, loss_fn=nn.L1Loss())\n",
        "mse_mae, r2_mae = evaluate_model(model_mae, test_loader)\n",
        "print(f\"With MAE: Test R2 {r2_mae:.4f}\")\n",
        "\n",
        "print(f\"\\nBest model: {best_model_name} with R2 {best_r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXGGeFyarY5b",
        "outputId": "6abaccd9-da83-491e-ecae-27e599e834a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Linear\n",
            "Epoch 50/500, Train Loss: 353.6811, Val Loss: 422.3233\n",
            "Early stopping at epoch 64\n",
            "Test MSE: 442.3259, Test R2: 0.4430\n",
            "Epoch 50/500, Train Loss: 12534.4308, Val Loss: 12063.7306\n",
            "Epoch 100/500, Train Loss: 2064.8459, Val Loss: 1935.7911\n",
            "Epoch 150/500, Train Loss: 357.3207, Val Loss: 415.2481\n",
            "Early stopping at epoch 189\n",
            "Test MSE: 440.8216, Test R2: 0.4449\n",
            "Epoch 50/500, Train Loss: 30192.8149, Val Loss: 29606.4165\n",
            "Epoch 100/500, Train Loss: 27788.7417, Val Loss: 27226.2938\n",
            "Epoch 150/500, Train Loss: 25497.7778, Val Loss: 24961.3845\n",
            "Epoch 200/500, Train Loss: 23317.4128, Val Loss: 22809.7394\n",
            "Epoch 250/500, Train Loss: 21242.9474, Val Loss: 20763.7374\n",
            "Epoch 300/500, Train Loss: 19268.2506, Val Loss: 18820.0108\n",
            "Epoch 350/500, Train Loss: 17395.3183, Val Loss: 16976.9704\n",
            "Epoch 400/500, Train Loss: 15616.0604, Val Loss: 15227.3852\n",
            "Epoch 450/500, Train Loss: 13939.3527, Val Loss: 13579.5818\n",
            "Epoch 500/500, Train Loss: 12362.4627, Val Loss: 12031.6218\n",
            "Test MSE: 12721.0387, Test R2: -15.0177\n",
            "Epoch 50/500, Train Loss: 32433.7152, Val Loss: 31859.1485\n",
            "Epoch 100/500, Train Loss: 32176.0083, Val Loss: 31602.1987\n",
            "Epoch 150/500, Train Loss: 31919.6490, Val Loss: 31346.6980\n",
            "Epoch 200/500, Train Loss: 31664.3596, Val Loss: 31092.4269\n",
            "Epoch 250/500, Train Loss: 31410.0411, Val Loss: 30839.2810\n",
            "Epoch 300/500, Train Loss: 31156.9275, Val Loss: 30587.5505\n",
            "Epoch 350/500, Train Loss: 30905.5518, Val Loss: 30337.5714\n",
            "Epoch 400/500, Train Loss: 30656.4119, Val Loss: 30089.9135\n",
            "Epoch 450/500, Train Loss: 30408.4404, Val Loss: 29843.5686\n",
            "Epoch 500/500, Train Loss: 30160.8756, Val Loss: 29597.7189\n",
            "Test MSE: 30713.7270, Test R2: -37.6731\n",
            "\n",
            "Training DNN-16\n",
            "Epoch 50/500, Train Loss: 360.6725, Val Loss: 481.5849\n",
            "Epoch 100/500, Train Loss: 350.3132, Val Loss: 468.5480\n",
            "Early stopping at epoch 122\n",
            "Test MSE: 495.1550, Test R2: 0.3765\n",
            "Epoch 50/500, Train Loss: 340.7345, Val Loss: 426.7462\n",
            "Epoch 100/500, Train Loss: 328.4399, Val Loss: 423.0839\n",
            "Epoch 150/500, Train Loss: 322.9906, Val Loss: 421.9465\n",
            "Early stopping at epoch 195\n",
            "Test MSE: 431.6057, Test R2: 0.4565\n",
            "Epoch 50/500, Train Loss: 19640.2810, Val Loss: 18619.3316\n",
            "Epoch 100/500, Train Loss: 5357.0333, Val Loss: 5709.0532\n",
            "Epoch 150/500, Train Loss: 1643.5761, Val Loss: 1892.8972\n",
            "Epoch 200/500, Train Loss: 652.0886, Val Loss: 784.3541\n",
            "Epoch 250/500, Train Loss: 428.6014, Val Loss: 527.7581\n",
            "Epoch 300/500, Train Loss: 366.3887, Val Loss: 454.9857\n",
            "Epoch 350/500, Train Loss: 345.5093, Val Loss: 431.8282\n",
            "Epoch 400/500, Train Loss: 338.4080, Val Loss: 423.7910\n",
            "Epoch 450/500, Train Loss: 334.8976, Val Loss: 423.1509\n",
            "Epoch 500/500, Train Loss: 332.6690, Val Loss: 422.1112\n",
            "Test MSE: 432.0659, Test R2: 0.4560\n",
            "Epoch 50/500, Train Loss: 32269.3026, Val Loss: 31688.6470\n",
            "Epoch 100/500, Train Loss: 31684.6321, Val Loss: 31088.4012\n",
            "Epoch 150/500, Train Loss: 30875.5096, Val Loss: 30250.5724\n",
            "Epoch 200/500, Train Loss: 29813.2726, Val Loss: 29146.9438\n",
            "Epoch 250/500, Train Loss: 28499.5183, Val Loss: 27782.9266\n",
            "Epoch 300/500, Train Loss: 26952.3179, Val Loss: 26181.5578\n",
            "Epoch 350/500, Train Loss: 25201.9085, Val Loss: 24378.9184\n",
            "Epoch 400/500, Train Loss: 23285.8018, Val Loss: 22420.8362\n",
            "Epoch 450/500, Train Loss: 21251.3547, Val Loss: 20361.1542\n",
            "Epoch 500/500, Train Loss: 19148.8413, Val Loss: 18257.6945\n",
            "Test MSE: 19227.2588, Test R2: -23.2099\n",
            "\n",
            "Training DNN-30-8\n",
            "Epoch 50/500, Train Loss: 491.5904, Val Loss: 639.1748\n",
            "Early stopping at epoch 73\n",
            "Test MSE: 554.2689, Test R2: 0.3021\n",
            "Epoch 50/500, Train Loss: 326.3649, Val Loss: 434.1376\n",
            "Epoch 100/500, Train Loss: 300.3686, Val Loss: 407.1253\n",
            "Epoch 150/500, Train Loss: 283.4080, Val Loss: 412.9016\n",
            "Early stopping at epoch 164\n",
            "Test MSE: 406.4557, Test R2: 0.4882\n",
            "Epoch 50/500, Train Loss: 5965.6985, Val Loss: 6041.0357\n",
            "Epoch 100/500, Train Loss: 612.4826, Val Loss: 735.3627\n",
            "Epoch 150/500, Train Loss: 370.3070, Val Loss: 457.3755\n",
            "Epoch 200/500, Train Loss: 339.0770, Val Loss: 433.5086\n",
            "Epoch 250/500, Train Loss: 328.8016, Val Loss: 423.2792\n",
            "Epoch 300/500, Train Loss: 322.8984, Val Loss: 420.9722\n",
            "Epoch 350/500, Train Loss: 319.1626, Val Loss: 420.1280\n",
            "Early stopping at epoch 377\n",
            "Test MSE: 420.1841, Test R2: 0.4709\n",
            "Epoch 50/500, Train Loss: 32403.0514, Val Loss: 31830.1942\n",
            "Epoch 100/500, Train Loss: 31909.2212, Val Loss: 31332.8045\n",
            "Epoch 150/500, Train Loss: 31013.2722, Val Loss: 30421.2132\n",
            "Epoch 200/500, Train Loss: 29476.4343, Val Loss: 28845.1931\n",
            "Epoch 250/500, Train Loss: 27090.1465, Val Loss: 26391.8444\n",
            "Epoch 300/500, Train Loss: 23751.4668, Val Loss: 22972.9439\n",
            "Epoch 350/500, Train Loss: 19553.0872, Val Loss: 18728.0510\n",
            "Epoch 400/500, Train Loss: 14872.8770, Val Loss: 14119.5653\n",
            "Epoch 450/500, Train Loss: 10399.7213, Val Loss: 9940.5406\n",
            "Epoch 500/500, Train Loss: 6979.3149, Val Loss: 7025.1034\n",
            "Test MSE: 7239.8235, Test R2: -8.1160\n",
            "\n",
            "Training DNN-30-16-8\n",
            "Epoch 50/500, Train Loss: 499.0710, Val Loss: 628.3256\n",
            "Epoch 100/500, Train Loss: 471.7575, Val Loss: 458.1975\n",
            "Early stopping at epoch 118\n",
            "Test MSE: 475.1094, Test R2: 0.4018\n",
            "Epoch 50/500, Train Loss: 306.9029, Val Loss: 410.4253\n",
            "Epoch 100/500, Train Loss: 264.9048, Val Loss: 424.1730\n",
            "Epoch 150/500, Train Loss: 242.2854, Val Loss: 411.8102\n",
            "Early stopping at epoch 172\n",
            "Test MSE: 426.6531, Test R2: 0.4628\n",
            "Epoch 50/500, Train Loss: 2565.6443, Val Loss: 2852.8721\n",
            "Epoch 100/500, Train Loss: 422.5995, Val Loss: 523.6283\n",
            "Epoch 150/500, Train Loss: 346.3144, Val Loss: 446.5862\n",
            "Epoch 200/500, Train Loss: 326.7764, Val Loss: 425.8090\n",
            "Epoch 250/500, Train Loss: 316.3283, Val Loss: 425.1682\n",
            "Epoch 300/500, Train Loss: 308.9338, Val Loss: 423.8006\n",
            "Epoch 350/500, Train Loss: 302.3381, Val Loss: 417.2335\n",
            "Epoch 400/500, Train Loss: 294.0927, Val Loss: 409.8995\n",
            "Epoch 450/500, Train Loss: 286.9194, Val Loss: 409.1897\n",
            "Epoch 500/500, Train Loss: 279.9451, Val Loss: 413.5522\n",
            "Test MSE: 414.3724, Test R2: 0.4782\n",
            "Epoch 50/500, Train Loss: 32501.8943, Val Loss: 31929.8054\n",
            "Epoch 100/500, Train Loss: 32180.9003, Val Loss: 31611.7453\n",
            "Epoch 150/500, Train Loss: 31756.3767, Val Loss: 31189.1388\n",
            "Epoch 200/500, Train Loss: 30917.7455, Val Loss: 30348.6897\n",
            "Epoch 250/500, Train Loss: 29110.1860, Val Loss: 28523.8738\n",
            "Epoch 300/500, Train Loss: 25639.9440, Val Loss: 25004.1728\n",
            "Epoch 350/500, Train Loss: 19981.3153, Val Loss: 19280.0486\n",
            "Epoch 400/500, Train Loss: 12545.4285, Val Loss: 11924.0175\n",
            "Epoch 450/500, Train Loss: 5955.7762, Val Loss: 5942.7201\n",
            "Epoch 500/500, Train Loss: 3497.0343, Val Loss: 3939.0255\n",
            "Test MSE: 3966.8701, Test R2: -3.9949\n",
            "\n",
            "Training DNN-30-16-8-4\n",
            "Epoch 50/500, Train Loss: 443.7708, Val Loss: 500.3862\n",
            "Epoch 100/500, Train Loss: 379.5690, Val Loss: 526.6952\n",
            "Early stopping at epoch 145\n",
            "Test MSE: 509.2390, Test R2: 0.3588\n",
            "Epoch 50/500, Train Loss: 283.5692, Val Loss: 404.9182\n",
            "Epoch 100/500, Train Loss: 244.3954, Val Loss: 414.4910\n",
            "Early stopping at epoch 113\n",
            "Test MSE: 448.6704, Test R2: 0.4351\n",
            "Epoch 50/500, Train Loss: 14943.0282, Val Loss: 13434.8144\n",
            "Epoch 100/500, Train Loss: 412.2614, Val Loss: 498.2207\n",
            "Epoch 150/500, Train Loss: 332.4233, Val Loss: 444.9986\n",
            "Epoch 200/500, Train Loss: 311.7831, Val Loss: 426.7980\n",
            "Epoch 250/500, Train Loss: 300.3386, Val Loss: 418.8070\n",
            "Epoch 300/500, Train Loss: 290.7406, Val Loss: 410.4715\n",
            "Epoch 350/500, Train Loss: 282.8685, Val Loss: 401.9969\n",
            "Epoch 400/500, Train Loss: 276.7924, Val Loss: 397.4091\n",
            "Epoch 450/500, Train Loss: 270.6076, Val Loss: 395.2169\n",
            "Epoch 500/500, Train Loss: 264.3902, Val Loss: 391.7335\n",
            "Test MSE: 404.5774, Test R2: 0.4906\n",
            "Epoch 50/500, Train Loss: 32552.2245, Val Loss: 31979.1770\n",
            "Epoch 100/500, Train Loss: 32161.5538, Val Loss: 31589.7355\n",
            "Epoch 150/500, Train Loss: 31380.8524, Val Loss: 30800.9549\n",
            "Epoch 200/500, Train Loss: 29524.5355, Val Loss: 28901.6988\n",
            "Epoch 250/500, Train Loss: 25303.7779, Val Loss: 24560.4463\n",
            "Epoch 300/500, Train Loss: 17468.6339, Val Loss: 16601.7047\n",
            "Epoch 350/500, Train Loss: 7809.8640, Val Loss: 7446.1195\n",
            "Epoch 400/500, Train Loss: 3351.0517, Val Loss: 3819.3184\n",
            "Epoch 450/500, Train Loss: 1850.0461, Val Loss: 2150.5648\n",
            "Epoch 500/500, Train Loss: 1159.7508, Val Loss: 1329.7704\n",
            "Test MSE: 1550.4040, Test R2: -0.9522\n",
            "\n",
            "Training DNN-128\n",
            "Epoch 50/500, Train Loss: 348.6720, Val Loss: 479.7044\n",
            "Epoch 100/500, Train Loss: 289.0851, Val Loss: 471.2187\n",
            "Early stopping at epoch 140\n",
            "Test MSE: 473.1280, Test R2: 0.4043\n",
            "Epoch 50/500, Train Loss: 338.8329, Val Loss: 441.8261\n",
            "Epoch 100/500, Train Loss: 318.1004, Val Loss: 423.0929\n",
            "Epoch 150/500, Train Loss: 310.9642, Val Loss: 424.3362\n",
            "Early stopping at epoch 184\n",
            "Test MSE: 431.0182, Test R2: 0.4573\n",
            "Epoch 50/500, Train Loss: 17096.4492, Val Loss: 16149.0556\n",
            "Epoch 100/500, Train Loss: 3897.5524, Val Loss: 4339.7163\n",
            "Epoch 150/500, Train Loss: 1313.7062, Val Loss: 1509.5288\n",
            "Epoch 200/500, Train Loss: 634.1878, Val Loss: 760.6257\n",
            "Epoch 250/500, Train Loss: 451.6660, Val Loss: 560.9049\n",
            "Epoch 300/500, Train Loss: 389.0822, Val Loss: 491.9621\n",
            "Epoch 350/500, Train Loss: 360.8756, Val Loss: 462.6087\n",
            "Epoch 400/500, Train Loss: 345.9748, Val Loss: 450.5688\n",
            "Epoch 450/500, Train Loss: 337.6935, Val Loss: 439.8512\n",
            "Epoch 500/500, Train Loss: 331.9942, Val Loss: 435.8405\n",
            "Test MSE: 441.6580, Test R2: 0.4439\n",
            "Epoch 50/500, Train Loss: 31901.4384, Val Loss: 31325.3980\n",
            "Epoch 100/500, Train Loss: 30991.7229, Val Loss: 30411.3233\n",
            "Epoch 150/500, Train Loss: 29915.4353, Val Loss: 29320.9696\n",
            "Epoch 200/500, Train Loss: 28637.2124, Val Loss: 28019.8925\n",
            "Epoch 250/500, Train Loss: 27147.8104, Val Loss: 26500.2281\n",
            "Epoch 300/500, Train Loss: 25459.5995, Val Loss: 24776.9365\n",
            "Epoch 350/500, Train Loss: 23597.0668, Val Loss: 22879.9046\n",
            "Epoch 400/500, Train Loss: 21595.2730, Val Loss: 20851.5463\n",
            "Epoch 450/500, Train Loss: 19495.3213, Val Loss: 18740.7604\n",
            "Epoch 500/500, Train Loss: 17349.4538, Val Loss: 16606.6936\n",
            "Test MSE: 17408.4481, Test R2: -20.9198\n",
            "\n",
            "Training DNN-256-128\n",
            "Epoch 50/500, Train Loss: 423.0586, Val Loss: 768.6890\n",
            "Early stopping at epoch 86\n",
            "Test MSE: 538.2641, Test R2: 0.3222\n",
            "Epoch 50/500, Train Loss: 296.6538, Val Loss: 417.0142\n",
            "Epoch 100/500, Train Loss: 236.5353, Val Loss: 387.8938\n",
            "Epoch 150/500, Train Loss: 197.3214, Val Loss: 386.4022\n",
            "Early stopping at epoch 186\n",
            "Test MSE: 424.1916, Test R2: 0.4659\n",
            "Epoch 50/500, Train Loss: 2894.6974, Val Loss: 3138.3805\n",
            "Epoch 100/500, Train Loss: 825.8968, Val Loss: 901.2270\n",
            "Epoch 150/500, Train Loss: 481.5492, Val Loss: 580.9724\n",
            "Epoch 200/500, Train Loss: 381.3675, Val Loss: 489.0949\n",
            "Epoch 250/500, Train Loss: 341.4931, Val Loss: 450.4581\n",
            "Epoch 300/500, Train Loss: 320.4819, Val Loss: 429.9357\n",
            "Epoch 350/500, Train Loss: 306.7478, Val Loss: 417.5946\n",
            "Epoch 400/500, Train Loss: 295.7840, Val Loss: 410.0081\n",
            "Epoch 450/500, Train Loss: 286.4019, Val Loss: 403.6325\n",
            "Epoch 500/500, Train Loss: 277.9224, Val Loss: 396.9278\n",
            "Test MSE: 407.5576, Test R2: 0.4868\n",
            "Epoch 50/500, Train Loss: 31905.0060, Val Loss: 31318.5923\n",
            "Epoch 100/500, Train Loss: 30272.1857, Val Loss: 29660.9714\n",
            "Epoch 150/500, Train Loss: 27656.7049, Val Loss: 27005.6056\n",
            "Epoch 200/500, Train Loss: 24063.1398, Val Loss: 23360.0103\n",
            "Epoch 250/500, Train Loss: 19619.9239, Val Loss: 18876.3664\n",
            "Epoch 300/500, Train Loss: 14672.0933, Val Loss: 13954.9002\n",
            "Epoch 350/500, Train Loss: 9850.5538, Val Loss: 9313.8945\n",
            "Epoch 400/500, Train Loss: 6047.6159, Val Loss: 5893.1230\n",
            "Epoch 450/500, Train Loss: 3930.1850, Val Loss: 4141.9601\n",
            "Epoch 500/500, Train Loss: 2979.7927, Val Loss: 3269.6459\n",
            "Test MSE: 3818.6046, Test R2: -3.8082\n",
            "\n",
            "Training DNN-512-256-128\n",
            "Epoch 50/500, Train Loss: 741.2327, Val Loss: 1312.2565\n",
            "Epoch 100/500, Train Loss: 395.3814, Val Loss: 474.8118\n",
            "Epoch 150/500, Train Loss: 296.0653, Val Loss: 460.5899\n",
            "Early stopping at epoch 163\n",
            "Test MSE: 485.2559, Test R2: 0.3890\n",
            "Epoch 50/500, Train Loss: 225.7230, Val Loss: 385.9114\n",
            "Epoch 100/500, Train Loss: 155.9122, Val Loss: 396.0680\n",
            "Early stopping at epoch 129\n",
            "Test MSE: 418.8525, Test R2: 0.4726\n",
            "Epoch 50/500, Train Loss: 2056.0228, Val Loss: 2029.7986\n",
            "Epoch 100/500, Train Loss: 623.3490, Val Loss: 722.7357\n",
            "Epoch 150/500, Train Loss: 333.1784, Val Loss: 475.9299\n",
            "Epoch 200/500, Train Loss: 275.7808, Val Loss: 422.8226\n",
            "Epoch 250/500, Train Loss: 250.3851, Val Loss: 402.1460\n",
            "Epoch 300/500, Train Loss: 233.6672, Val Loss: 393.9729\n",
            "Epoch 350/500, Train Loss: 218.8305, Val Loss: 396.0901\n",
            "Epoch 400/500, Train Loss: 206.4856, Val Loss: 394.9483\n",
            "Epoch 450/500, Train Loss: 194.9882, Val Loss: 390.2133\n",
            "Epoch 500/500, Train Loss: 184.7331, Val Loss: 391.0513\n",
            "Test MSE: 411.5603, Test R2: 0.4818\n",
            "Epoch 50/500, Train Loss: 31968.0325, Val Loss: 31382.0863\n",
            "Epoch 100/500, Train Loss: 29670.7485, Val Loss: 29052.9061\n",
            "Epoch 150/500, Train Loss: 24751.0113, Val Loss: 24084.0648\n",
            "Epoch 200/500, Train Loss: 17101.4599, Val Loss: 16412.8307\n",
            "Epoch 250/500, Train Loss: 8626.5007, Val Loss: 8105.5764\n",
            "Epoch 300/500, Train Loss: 4150.1291, Val Loss: 4156.2489\n",
            "Epoch 350/500, Train Loss: 3074.9936, Val Loss: 3192.2090\n",
            "Epoch 400/500, Train Loss: 2464.7068, Val Loss: 2571.3665\n",
            "Epoch 450/500, Train Loss: 2058.9811, Val Loss: 2148.0441\n",
            "Epoch 500/500, Train Loss: 1759.4716, Val Loss: 1833.5200\n",
            "Test MSE: 2231.0877, Test R2: -1.8093\n",
            "\n",
            "Training DNN-1024-512-256-128\n",
            "Epoch 50/500, Train Loss: 1308.1374, Val Loss: 924.4824\n",
            "Epoch 100/500, Train Loss: 288.1667, Val Loss: 449.3856\n",
            "Epoch 150/500, Train Loss: 207.9617, Val Loss: 425.0977\n",
            "Early stopping at epoch 166\n",
            "Test MSE: 592.4333, Test R2: 0.2540\n",
            "Epoch 50/500, Train Loss: 214.1136, Val Loss: 466.2472\n",
            "Epoch 100/500, Train Loss: 150.4261, Val Loss: 405.3430\n",
            "Early stopping at epoch 109\n",
            "Test MSE: 439.1541, Test R2: 0.4470\n",
            "Epoch 50/500, Train Loss: 1634.9933, Val Loss: 1675.6385\n",
            "Epoch 100/500, Train Loss: 389.7540, Val Loss: 542.3842\n",
            "Epoch 150/500, Train Loss: 279.7777, Val Loss: 438.1279\n",
            "Epoch 200/500, Train Loss: 242.4004, Val Loss: 405.7511\n",
            "Epoch 250/500, Train Loss: 218.2019, Val Loss: 401.1919\n",
            "Epoch 300/500, Train Loss: 197.4787, Val Loss: 401.8340\n",
            "Epoch 350/500, Train Loss: 179.9825, Val Loss: 399.1248\n",
            "Epoch 400/500, Train Loss: 163.2654, Val Loss: 395.8588\n",
            "Early stopping at epoch 427\n",
            "Test MSE: 431.1988, Test R2: 0.4571\n",
            "Epoch 50/500, Train Loss: 32061.7663, Val Loss: 31478.1310\n",
            "Epoch 100/500, Train Loss: 29364.3073, Val Loss: 28729.5514\n",
            "Epoch 150/500, Train Loss: 21670.8277, Val Loss: 20949.5555\n",
            "Epoch 200/500, Train Loss: 9628.0443, Val Loss: 9008.6132\n",
            "Epoch 250/500, Train Loss: 4846.7910, Val Loss: 4723.2100\n",
            "Epoch 300/500, Train Loss: 3606.8627, Val Loss: 3583.3778\n",
            "Epoch 350/500, Train Loss: 2806.3313, Val Loss: 2818.2818\n",
            "Epoch 400/500, Train Loss: 2254.6135, Val Loss: 2271.1885\n",
            "Epoch 450/500, Train Loss: 1854.2461, Val Loss: 1868.6244\n",
            "Epoch 500/500, Train Loss: 1545.7641, Val Loss: 1561.8602\n",
            "Test MSE: 1878.4623, Test R2: -1.3653\n",
            "\n",
            "Training DNN-32x4\n",
            "Epoch 50/500, Train Loss: 454.9628, Val Loss: 674.5838\n",
            "Epoch 100/500, Train Loss: 392.4634, Val Loss: 442.0829\n",
            "Early stopping at epoch 150\n",
            "Test MSE: 598.8378, Test R2: 0.2460\n",
            "Epoch 50/500, Train Loss: 292.4349, Val Loss: 408.2645\n",
            "Early stopping at epoch 91\n",
            "Test MSE: 427.9823, Test R2: 0.4611\n",
            "Epoch 50/500, Train Loss: 1042.9982, Val Loss: 1130.6330\n",
            "Epoch 100/500, Train Loss: 376.7073, Val Loss: 470.7617\n",
            "Epoch 150/500, Train Loss: 329.1819, Val Loss: 427.7435\n",
            "Epoch 200/500, Train Loss: 312.4963, Val Loss: 417.1988\n",
            "Epoch 250/500, Train Loss: 300.4140, Val Loss: 409.3324\n",
            "Epoch 300/500, Train Loss: 290.9638, Val Loss: 406.3444\n",
            "Epoch 350/500, Train Loss: 283.0063, Val Loss: 397.0980\n",
            "Epoch 400/500, Train Loss: 274.8265, Val Loss: 399.3026\n",
            "Early stopping at epoch 421\n",
            "Test MSE: 437.8142, Test R2: 0.4487\n",
            "Epoch 50/500, Train Loss: 32231.1100, Val Loss: 31659.5893\n",
            "Epoch 100/500, Train Loss: 31593.4054, Val Loss: 31020.7013\n",
            "Epoch 150/500, Train Loss: 30212.7843, Val Loss: 29624.0291\n",
            "Epoch 200/500, Train Loss: 27055.0855, Val Loss: 26404.6679\n",
            "Epoch 250/500, Train Loss: 20727.3009, Val Loss: 19946.4920\n",
            "Epoch 300/500, Train Loss: 11079.0392, Val Loss: 10329.5594\n",
            "Epoch 350/500, Train Loss: 3839.9131, Val Loss: 3979.5570\n",
            "Epoch 400/500, Train Loss: 2047.8269, Val Loss: 2269.6139\n",
            "Epoch 450/500, Train Loss: 1327.9637, Val Loss: 1459.3145\n",
            "Epoch 500/500, Train Loss: 977.5094, Val Loss: 1078.3774\n",
            "Test MSE: 1168.3076, Test R2: -0.4711\n",
            "\n",
            "Training DNN-16x5\n",
            "Epoch 50/500, Train Loss: 500.9041, Val Loss: 703.3794\n",
            "Early stopping at epoch 90\n",
            "Test MSE: 625.6968, Test R2: 0.2122\n",
            "Epoch 50/500, Train Loss: 284.6743, Val Loss: 433.2640\n",
            "Early stopping at epoch 78\n",
            "Test MSE: 454.3077, Test R2: 0.4280\n",
            "Epoch 50/500, Train Loss: 505.3952, Val Loss: 534.5980\n",
            "Epoch 100/500, Train Loss: 331.8888, Val Loss: 445.1558\n",
            "Epoch 150/500, Train Loss: 302.8872, Val Loss: 411.4769\n",
            "Epoch 200/500, Train Loss: 287.6414, Val Loss: 402.5586\n",
            "Epoch 250/500, Train Loss: 276.9625, Val Loss: 399.9150\n",
            "Early stopping at epoch 295\n",
            "Test MSE: 413.2120, Test R2: 0.4797\n",
            "Epoch 50/500, Train Loss: 32358.6136, Val Loss: 31787.5529\n",
            "Epoch 100/500, Train Loss: 31974.0454, Val Loss: 31405.7538\n",
            "Epoch 150/500, Train Loss: 31389.1234, Val Loss: 30823.9361\n",
            "Epoch 200/500, Train Loss: 30377.4149, Val Loss: 29817.4178\n",
            "Epoch 250/500, Train Loss: 28549.2935, Val Loss: 27999.1656\n",
            "Epoch 300/500, Train Loss: 25063.4797, Val Loss: 24530.8847\n",
            "Epoch 350/500, Train Loss: 18332.9802, Val Loss: 17825.6299\n",
            "Epoch 400/500, Train Loss: 7539.0113, Val Loss: 7145.8586\n",
            "Epoch 450/500, Train Loss: 1293.7541, Val Loss: 1410.1795\n",
            "Epoch 500/500, Train Loss: 640.0242, Val Loss: 761.7554\n",
            "Test MSE: 759.6761, Test R2: 0.0435\n",
            "\n",
            "Training DNN-8x6\n",
            "Epoch 50/500, Train Loss: 523.3819, Val Loss: 626.5101\n",
            "Early stopping at epoch 88\n",
            "Test MSE: 558.3422, Test R2: 0.2970\n",
            "Epoch 50/500, Train Loss: 302.8999, Val Loss: 413.3867\n",
            "Early stopping at epoch 74\n",
            "Test MSE: 466.5055, Test R2: 0.4126\n",
            "Epoch 50/500, Train Loss: 382.8213, Val Loss: 446.9625\n",
            "Epoch 100/500, Train Loss: 331.9995, Val Loss: 418.4083\n",
            "Epoch 150/500, Train Loss: 322.2981, Val Loss: 410.2953\n",
            "Epoch 200/500, Train Loss: 314.1588, Val Loss: 397.1916\n",
            "Epoch 250/500, Train Loss: 308.2326, Val Loss: 393.7662\n",
            "Epoch 300/500, Train Loss: 301.4724, Val Loss: 388.9115\n",
            "Epoch 350/500, Train Loss: 296.5851, Val Loss: 387.0232\n",
            "Early stopping at epoch 389\n",
            "Test MSE: 424.5343, Test R2: 0.4654\n",
            "Epoch 50/500, Train Loss: 32200.1928, Val Loss: 31629.2053\n",
            "Epoch 100/500, Train Loss: 31581.1225, Val Loss: 31013.3658\n",
            "Epoch 150/500, Train Loss: 30515.7759, Val Loss: 29952.2049\n",
            "Epoch 200/500, Train Loss: 28481.5856, Val Loss: 27924.8916\n",
            "Epoch 250/500, Train Loss: 24450.5759, Val Loss: 23903.8858\n",
            "Epoch 300/500, Train Loss: 16668.7801, Val Loss: 16121.2425\n",
            "Epoch 350/500, Train Loss: 4787.3178, Val Loss: 4339.3073\n",
            "Epoch 400/500, Train Loss: 609.4938, Val Loss: 588.7580\n",
            "Epoch 450/500, Train Loss: 447.5603, Val Loss: 471.6931\n",
            "Epoch 500/500, Train Loss: 398.0170, Val Loss: 452.4278\n",
            "Test MSE: 495.5855, Test R2: 0.3760\n",
            "\n",
            "Training DNN-64-32-16-8-4\n",
            "Epoch 50/500, Train Loss: 433.2409, Val Loss: 474.6730\n",
            "Early stopping at epoch 88\n",
            "Test MSE: 573.6410, Test R2: 0.2777\n",
            "Epoch 50/500, Train Loss: 261.2836, Val Loss: 454.9441\n",
            "Epoch 100/500, Train Loss: 208.7945, Val Loss: 460.9448\n",
            "Early stopping at epoch 111\n",
            "Test MSE: 427.6814, Test R2: 0.4615\n",
            "Epoch 50/500, Train Loss: 1710.4796, Val Loss: 1750.9532\n",
            "Epoch 100/500, Train Loss: 365.2774, Val Loss: 481.5872\n",
            "Epoch 150/500, Train Loss: 306.4194, Val Loss: 424.7549\n",
            "Epoch 200/500, Train Loss: 286.5693, Val Loss: 402.5671\n",
            "Epoch 250/500, Train Loss: 269.7899, Val Loss: 398.3311\n",
            "Epoch 300/500, Train Loss: 256.3038, Val Loss: 396.9857\n",
            "Epoch 350/500, Train Loss: 245.6016, Val Loss: 383.3429\n",
            "Epoch 400/500, Train Loss: 235.5272, Val Loss: 383.4358\n",
            "Epoch 450/500, Train Loss: 225.1780, Val Loss: 388.3303\n",
            "Epoch 500/500, Train Loss: 216.2119, Val Loss: 380.3608\n",
            "Test MSE: 411.8069, Test R2: 0.4815\n",
            "Epoch 50/500, Train Loss: 32421.2938, Val Loss: 31848.2628\n",
            "Epoch 100/500, Train Loss: 31818.2772, Val Loss: 31248.1086\n",
            "Epoch 150/500, Train Loss: 30784.9578, Val Loss: 30218.2046\n",
            "Epoch 200/500, Train Loss: 28895.0097, Val Loss: 28329.2591\n",
            "Epoch 250/500, Train Loss: 24995.2015, Val Loss: 24415.6235\n",
            "Epoch 300/500, Train Loss: 16761.2586, Val Loss: 16149.1507\n",
            "Epoch 350/500, Train Loss: 4815.9674, Val Loss: 4469.2923\n",
            "Epoch 400/500, Train Loss: 1640.9696, Val Loss: 1749.0231\n",
            "Epoch 450/500, Train Loss: 1027.2650, Val Loss: 1113.6468\n",
            "Epoch 500/500, Train Loss: 751.2141, Val Loss: 825.0960\n",
            "Test MSE: 832.4522, Test R2: -0.0482\n",
            "\n",
            "Step 2 Results (R2 for LR=0.01):\n",
            "Linear: 0.4449\n",
            "DNN-16: 0.4565\n",
            "DNN-30-8: 0.4882\n",
            "DNN-30-16-8: 0.4628\n",
            "DNN-30-16-8-4: 0.4351\n",
            "DNN-128: 0.4573\n",
            "DNN-256-128: 0.4659\n",
            "DNN-512-256-128: 0.4726\n",
            "DNN-1024-512-256-128: 0.4470\n",
            "DNN-32x4: 0.4611\n",
            "DNN-16x5: 0.4280\n",
            "DNN-8x6: 0.4126\n",
            "DNN-64-32-16-8-4: 0.4615\n",
            "\n",
            "Step 5 Results (R2 for different LR):\n",
            "Linear\n",
            "LR 0.1: 0.4430\n",
            "LR 0.01: 0.4449\n",
            "LR 0.001: -15.0177\n",
            "LR 0.0001: -37.6731\n",
            "DNN-16\n",
            "LR 0.1: 0.3765\n",
            "LR 0.01: 0.4565\n",
            "LR 0.001: 0.4560\n",
            "LR 0.0001: -23.2099\n",
            "DNN-30-8\n",
            "LR 0.1: 0.3021\n",
            "LR 0.01: 0.4882\n",
            "LR 0.001: 0.4709\n",
            "LR 0.0001: -8.1160\n",
            "DNN-30-16-8\n",
            "LR 0.1: 0.4018\n",
            "LR 0.01: 0.4628\n",
            "LR 0.001: 0.4782\n",
            "LR 0.0001: -3.9949\n",
            "DNN-30-16-8-4\n",
            "LR 0.1: 0.3588\n",
            "LR 0.01: 0.4351\n",
            "LR 0.001: 0.4906\n",
            "LR 0.0001: -0.9522\n",
            "DNN-128\n",
            "LR 0.1: 0.4043\n",
            "LR 0.01: 0.4573\n",
            "LR 0.001: 0.4439\n",
            "LR 0.0001: -20.9198\n",
            "DNN-256-128\n",
            "LR 0.1: 0.3222\n",
            "LR 0.01: 0.4659\n",
            "LR 0.001: 0.4868\n",
            "LR 0.0001: -3.8082\n",
            "DNN-512-256-128\n",
            "LR 0.1: 0.3890\n",
            "LR 0.01: 0.4726\n",
            "LR 0.001: 0.4818\n",
            "LR 0.0001: -1.8093\n",
            "DNN-1024-512-256-128\n",
            "LR 0.1: 0.2540\n",
            "LR 0.01: 0.4470\n",
            "LR 0.001: 0.4571\n",
            "LR 0.0001: -1.3653\n",
            "DNN-32x4\n",
            "LR 0.1: 0.2460\n",
            "LR 0.01: 0.4611\n",
            "LR 0.001: 0.4487\n",
            "LR 0.0001: -0.4711\n",
            "DNN-16x5\n",
            "LR 0.1: 0.2122\n",
            "LR 0.01: 0.4280\n",
            "LR 0.001: 0.4797\n",
            "LR 0.0001: 0.0435\n",
            "DNN-8x6\n",
            "LR 0.1: 0.2970\n",
            "LR 0.01: 0.4126\n",
            "LR 0.001: 0.4654\n",
            "LR 0.0001: 0.3760\n",
            "DNN-64-32-16-8-4\n",
            "LR 0.1: 0.2777\n",
            "LR 0.01: 0.4615\n",
            "LR 0.001: 0.4815\n",
            "LR 0.0001: -0.0482\n",
            "\n",
            "Step 6 MSE (for LR=0.01):\n",
            "Linear: 440.8216\n",
            "DNN-16: 431.6057\n",
            "DNN-30-8: 406.4557\n",
            "DNN-30-16-8: 426.6531\n",
            "DNN-30-16-8-4: 448.6704\n",
            "DNN-128: 431.0182\n",
            "DNN-256-128: 424.1916\n",
            "DNN-512-256-128: 418.8525\n",
            "DNN-1024-512-256-128: 439.1541\n",
            "DNN-32x4: 427.9823\n",
            "DNN-16x5: 454.3077\n",
            "DNN-8x6: 466.5055\n",
            "DNN-64-32-16-8-4: 427.6814\n",
            "\n",
            "Trying MAE loss for DNN-30-16-8\n",
            "Epoch 50/500, Train Loss: 13.1171, Val Loss: 15.0159\n",
            "Epoch 100/500, Train Loss: 12.0816, Val Loss: 14.4675\n",
            "Epoch 150/500, Train Loss: 11.6049, Val Loss: 14.5566\n",
            "Epoch 200/500, Train Loss: 11.3443, Val Loss: 14.4367\n",
            "Early stopping at epoch 203\n",
            "Test MSE: 427.3492, Test R2: 0.4619\n",
            "With MAE: Test R2 0.4619\n",
            "\n",
            "Best model: DNN-30-8 with R2 0.4882\n"
          ]
        }
      ]
    }
  ]
}